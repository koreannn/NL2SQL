{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63134c90",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d56eba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 1. 설정 (Configuration)\n",
    "# ==========================================\n",
    "DATA_PATH = \"/content/drive/MyDrive/nlp_project2/train_dataset_final.jsonl\"\n",
    "MODEL_NAME = \"KETI-AIR/ke-t5-base\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/nlp_project2/ke-t5_checkpoints\" # 체크포인트 저장 경로\n",
    "FINAL_SAVE_DIR = \"/content/drive/MyDrive/nlp_project2/final_ke-t5_model\" # 최종 모델 저장 경로\n",
    "\n",
    "# 입력/출력 최대 길이 설정 (데이터 길이에 따라 조절 가능)\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 128\n",
    "\n",
    "# ==========================================\n",
    "# 2. 데이터셋 로드 및 전처리\n",
    "# ==========================================\n",
    "# JSONL 파일 로드\n",
    "dataset = load_dataset(\"json\", data_files=DATA_PATH)\n",
    "\n",
    "# 학습/검증 데이터 분리 (예: 9:1), 데이터가 적다면 분리하지 않고 전체를 학습에 쓸 수도 있습니다.\n",
    "# 여기서는 전체 데이터를 학습용으로 쓴다고 가정하거나, 필요시 아래 주석을 해제하세요.\n",
    "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    targets = examples[\"target\"]\n",
    "\n",
    "    # 입력 토크나이징\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # 출력(타겟) 토크나이징\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    # 패딩 토큰의 ID를 -100으로 치환하여 손실(Loss) 계산에서 제외\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# 데이터셋에 전처리 적용\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# ==========================================\n",
    "# 3. 모델 불러오기\n",
    "# ==========================================\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ==========================================\n",
    "# 4. 학습 인자 설정 (A100 최적화)\n",
    "# ==========================================\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"steps\",     # 10000 steps마다 평가\n",
    "    eval_steps=10000,\n",
    "    save_strategy=\"steps\",            # 10000 steps마다 체크포인트 저장\n",
    "    save_steps=10000,\n",
    "    save_total_limit=3,               # 최근 3개의 체크포인트만 유지 (용량 절약)\n",
    "    learning_rate=2e-4,               # T5는 보통 1e-4 ~ 3e-4 정도 사용\n",
    "    per_device_train_batch_size=32,   # A100 메모리가 크므로 32~64 가능 (OOM 발생 시 줄이세요)\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,              # 학습 에폭 수 (데이터 양에 따라 조절)\n",
    "    predict_with_generate=True,       # 평가 시 생성 모드 사용\n",
    "    fp16=False,                       # A100에서는 bf16이 더 좋음\n",
    "    bf16=True,                        # A100 전용 설정 (Brain Float 16)\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=500,\n",
    "    report_to=\"wandb\",\n",
    "    load_best_model_at_end=True,      # 학습 종료 후 가장 성능 좋은 모델 로드\n",
    "    dataloader_num_workers=4,\n",
    ")\n",
    "\n",
    "# Data Collator 설정\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# ==========================================\n",
    "# 5. Trainer 초기화 및 학습 시작\n",
    "# ==========================================\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"학습을 시작합니다...\")\n",
    "trainer.train()\n",
    "\n",
    "# ==========================================\n",
    "# 6. 최종 모델 저장\n",
    "# ==========================================\n",
    "print(f\"학습 완료! 모델을 {FINAL_SAVE_DIR}에 저장합니다.\")\n",
    "trainer.save_model(FINAL_SAVE_DIR)\n",
    "tokenizer.save_pretrained(FINAL_SAVE_DIR)\n",
    "\n",
    "print(\"모든 작업이 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ef0606",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 저장된 모델 불러오기\n",
    "final_model_path = \"/content/drive/MyDrive/nlp_project2/final_ke-t5_model\"\n",
    "generator = pipeline(\"text2text-generation\", model=final_model_path, tokenizer=final_model_path, device=0)\n",
    "\n",
    "# 테스트 입력 (데이터셋 포맷에 맞춤)\n",
    "test_input = \"translate to sql:\\nquestion <3층메디칼약국의 주소를 알려줘>\\nschema <table TB_PHARMACY_OPERATE_INFO columns DUTYADDR(주소), DUTYNAME(약국명), HPID(약국아이디), DUTYTEL1(대표전화), DUTYTIME1C(월요일 진료 마감 시간), DUTYTIME2C(화요일 진료 마감 시간), DUTYTIME3C(수요일 진료 마감 시간), DUTYTIME4C(목요일 진료 마감 시간)>\"\n",
    "\n",
    "# 예측 생성\n",
    "result = generator(test_input, max_length=128)\n",
    "print(\"Input:\", test_input)\n",
    "print(\"Generated SQL:\", result[0]['generated_text'])\n",
    "print(\"정답 SQL: SELECT DUTYADDR FROM TB_PHARMACY_OPERATE_INFO WHERE DUTYNAME = '3층메디칼약국'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9cfa76",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
